{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bruno Ugolini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "challenging-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-transparency",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis\n",
    "\n",
    "Using the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), we want to do a regression model that predict the ratings are on a 1-10 scale. You have an example train and test set in the `dataset` folder.\n",
    "\n",
    "### 1.1 Regression Model\n",
    "\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset\n",
    "\n",
    "### 1.2 RNN model\n",
    "\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-spirituality",
   "metadata": {},
   "source": [
    "### Read in the data for review and ratings separated by train and test directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brave-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data_dir, sub):\n",
    "    \"\"\"\n",
    "    Find the data and read it into a dataframe\n",
    "    \"\"\"\n",
    "    files = os.listdir(path=data_dir+sub)\n",
    "    rating = []\n",
    "    txt = []\n",
    "    for file in files:\n",
    "        rating.append(file.split('_')[-1].split('.')[0])\n",
    "        with open(data_dir+sub+'\\\\'+file, mode='r', encoding='UTF-8') as f:\n",
    "            txt.append(f.read())\n",
    "    return rating, txt\n",
    "\n",
    "ratings = []\n",
    "txt = []\n",
    "train_dir = 'D:\\\\Shopee_Data\\\\aclImdb\\\\train\\\\'\n",
    "for sub in ['pos', 'neg']:\n",
    "    r, t = build_dataset(train_dir, sub)\n",
    "    ratings.extend(r)\n",
    "    txt.extend(t)\n",
    "\n",
    "df_train = pd.DataFrame(data=np.transpose([ratings, txt]),columns=['rating','review'])\n",
    "df_train.rating = df_train.rating.astype(float)\n",
    "\n",
    "ratings = []\n",
    "txt = []\n",
    "test_dir = 'D:\\\\Shopee_Data\\\\aclImdb\\\\test\\\\'\n",
    "for sub in ['pos', 'neg']:\n",
    "    r, t = build_dataset(test_dir, sub)\n",
    "    ratings.extend(r)\n",
    "    txt.extend(t)\n",
    "    \n",
    "df_test = pd.DataFrame(data=np.transpose([ratings, txt]),columns=['rating','review'])\n",
    "df_test.rating = df_test.rating.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spare-compilation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>3.0</td>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>3.0</td>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review\n",
       "0         9.0  Bromwell High is a cartoon comedy. It ran at t...\n",
       "1         8.0  Homelessness (or Houselessness as George Carli...\n",
       "2        10.0  Brilliant over-acting by Lesley Ann Warren. Be...\n",
       "3         7.0  This is easily the most underrated film inn th...\n",
       "4         8.0  This is not the typical Mel Brooks film. It wa...\n",
       "...       ...                                                ...\n",
       "24995     4.0  Towards the end of the movie, I felt it was to...\n",
       "24996     3.0  This is the kind of movie that my enemies cont...\n",
       "24997     3.0  I saw 'Descent' last night at the Stockholm Fi...\n",
       "24998     1.0  Some films that you pick up for a pound turn o...\n",
       "24999     1.0  This is one of the dumbest films, I've ever se...\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-berkeley",
   "metadata": {},
   "source": [
    "# Question 1.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-banana",
   "metadata": {},
   "source": [
    "### Select a subset of the training dataset to train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "basic-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 3_000\n",
    "train_sents = df_train.sample(N_TRAIN,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "muslim-beijing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare distributions of ratings\n",
    "# in sample vs. total\n",
    "train_sents.rating.hist();\n",
    "plt.show()\n",
    "df_train.rating.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-chance",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recovered-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "train_sents['tokens'] = train_sents['review'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "official-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['tokens'] = df_test['review'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-reflection",
   "metadata": {},
   "source": [
    "### Find the Parts of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "artificial-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract pos from the tuple of (word, POS)\n",
    "def split_row(row):\n",
    "    dets = []\n",
    "    for tpl in row:\n",
    "        t, d = tpl\n",
    "        dets.append(d)\n",
    "    return dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "third-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents['POS'] = train_sents['tokens'].apply(nltk.pos_tag)\n",
    "train_sents['pos'] = train_sents['POS'].apply(split_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arranged-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['POS'] = df_test['tokens'].apply(nltk.pos_tag)\n",
    "df_test['pos'] = df_test['POS'].apply(split_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = train_sents.drop('POS',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automotive-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop('POS',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-range",
   "metadata": {},
   "source": [
    "### Following the lectures notes for the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "requested-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      "LEXICON SAMPLE (42386 total items):\n",
      "{'In': 2, 'Panic': 3, 'The': 4, 'Streets': 5, 'Richard': 6, 'Widmark': 7, 'plays': 8, 'U.S.': 9, 'Navy': 10, 'doctor': 11, 'who': 12, 'has': 13, 'his': 14, 'week': 15, 'rudely': 16, 'interrupted': 17, 'with': 18, 'a': 19, 'corpse': 20, 'that': 21}\n",
      "TAGS:\n",
      "LEXICON SAMPLE (46 total items):\n",
      "{'IN': 2, 'NNP': 3, 'DT': 4, 'VBZ': 5, 'NN': 6, 'WP': 7, 'PRP$': 8, 'RB': 9, 'VBN': 10, 'WDT': 11, '.': 12, 'RP': 13, 'VBD': 14, 'CD': 15, 'NNS': 16, ',': 17, 'PRP': 18, 'VBP': 19, 'JJ': 20, 'TO': 21}\n"
     ]
    }
   ],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    '''Create a lexicon for the words in the sentences as well as the tags'''\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "print(\"WORDS:\")\n",
    "words_lexicon = make_lexicon(train_sents['tokens'])\n",
    "print(\"TAGS:\")\n",
    "tags_lexicon = make_lexicon(train_sents['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "romance-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_lookup(lexicon):\n",
    "    '''Make a dictionary where the string representation \n",
    "        of a lexicon item can be retrieved \n",
    "        from its numerical index\n",
    "    '''\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['tokens'], words_lexicon)\n",
    "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['pos'], tags_lexicon)\n",
    "\n",
    "# tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "documentary-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "built-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
    "max_seq_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "protected-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[   35    36    37 ...    80   258    24]\n",
      " [    0     0     0 ...   314   280    24]\n",
      " [    0     0     0 ...   213   361    24]\n",
      " ...\n",
      " [    0     0     0 ...    32   290    24]\n",
      " [    0     0     0 ... 28277   626    24]\n",
      " [    0     0     0 ... 16914  3029    24]]\n",
      "SHAPE: (3000, 501) \n",
      "\n",
      "TAGS:\n",
      " [[ 2 15 16 ... 20 16 12]\n",
      " [ 0  0  0 ... 20  6 12]\n",
      " [ 0  0  0 ...  9 20 12]\n",
      " ...\n",
      " [ 0  0  0 ...  4  6 12]\n",
      " [ 0  0  0 ...  6 38 12]\n",
      " [ 0  0  0 ... 20  6 12]]\n",
      "SHAPE: (3000, 501) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", train_padded_tags)\n",
    "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aware-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "failing-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer')\n",
    "    tag_input = Input(batch_shape=(batch_size, seq_input_len), name='tag_input_layer')\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True, name='word_embedding_layer')(word_input) #mask_zero will ignore 0 padding\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "    merged_embeddings = Concatenate(axis=-1, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = LSTM(units=n_hidden_nodes, return_sequences=False, \n",
    "                       stateful=stateful, name='hidden_layer')(merged_embeddings)\n",
    "#     hidden_layer = GRU(units=n_hidden_nodes, return_sequences=False, \n",
    "#                        stateful=stateful, name='hidden_layer')(merged_embeddings)\n",
    "#     hidden_layer = GRU(units=n_hidden_nodes, return_sequences=True, \n",
    "#                        stateful=stateful, name='hidden_layer')(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    output_layer = Dense(10,activation='softmax', name='output_layer')(hidden_layer)\n",
    "#     output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, \n",
    "#                                          activation='softmax'), name='output_layer')(hidden_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "grand-power",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42387, 47)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_lexicon) + 1, len(tags_lexicon) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intended-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input_layer (InputLayer)   [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tag_input_layer (InputLayer)    [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 500, 300)     12716100    word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tag_embedding_layer (Embedding) (None, 500, 100)     4700        tag_input_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_layer (Concate (None, 500, 400)     0           word_embedding_layer[0][0]       \n",
      "                                                                 tag_embedding_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (LSTM)             (None, 500)          1802000     concat_embedding_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 10)           5010        hidden_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 14,527,810\n",
      "Trainable params: 14,527,810\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=300,\n",
    "                     n_tag_embedding_nodes=100,\n",
    "                     n_hidden_nodes=500)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "accessory-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "proof-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(train_sents['rating']-1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "floating-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "94/94 [==============================] - 137s 1s/step - loss: 2.0721\n",
      "Epoch 2/8\n",
      "94/94 [==============================] - 138s 1s/step - loss: 1.9234\n",
      "Epoch 3/8\n",
      "94/94 [==============================] - 139s 1s/step - loss: 1.3117\n",
      "Epoch 4/8\n",
      "94/94 [==============================] - 137s 1s/step - loss: 0.6624\n",
      "Epoch 5/8\n",
      "94/94 [==============================] - 138s 1s/step - loss: 0.2905\n",
      "Epoch 6/8\n",
      "94/94 [==============================] - 136s 1s/step - loss: 0.1606\n",
      "Epoch 7/8\n",
      "94/94 [==============================] - 137s 1s/step - loss: 0.0603\n",
      "Epoch 8/8\n",
      "94/94 [==============================] - 136s 1s/step - loss: 0.0632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbbb0691f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
    "#           y=train_sents['rating'][:,].to_numpy(), \n",
    "          y=y_cat, \n",
    "          batch_size=32, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "suitable-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "conventional-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents['Sentence_Idxs'] = tokens_to_idxs(test_sents['tokens'], words_lexicon)\n",
    "test_sents['Tag_Idxs'] = tokens_to_idxs(test_sents['pos'], tags_lexicon)\n",
    "\n",
    "test_padded_words = pad_idx_seqs(test_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "test_padded_tags = pad_idx_seqs(test_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "buried-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vec = model.predict([test_padded_words[:,1:], test_padded_tags[:,:-1]], \n",
    "                           batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "modified-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_vec,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "confident-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "modular-string",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29744"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test['rating'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-fiber",
   "metadata": {},
   "source": [
    "# Question 1.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-stable",
   "metadata": {},
   "source": [
    "### Select a subset of the training dataset to train the FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "whole-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 3_000\n",
    "train_sents = df_train.sample(N_TRAIN,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "worst-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "surprised-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:\\Users\\Pinhead\\anaconda3\\envs\\tf-\n",
      "[nltk_data]     gpu\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\Users\\Pinhead\\anaconda3\\envs\\tf-\n",
      "[nltk_data]     gpu\\lib\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet');\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-steam",
   "metadata": {},
   "source": [
    "### Custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cultural-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "atomic-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "reported-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_from_stop = ['above', 'below', 'up', 'down', 'over', 'under', 'again', \n",
    "                       'more', 'most', 'no', 'not', 'only', 'too', 'very', \"do\", \n",
    "                       \"don't\", 'should', 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", \n",
    "                       'weren', \"weren't\",'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pressing-terry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'to', 'from', 'in', 'out', 'on', 'off', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'other', 'some', 'such', 'nor', 'own', 'same', 'so', 'than', 's', 't', 'can', 'will', 'just', 'don', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\"]\n"
     ]
    }
   ],
   "source": [
    "my_stop_words = stopwords.words('English')\n",
    "for w in to_remove_from_stop:\n",
    "    my_stop_words.remove(w)\n",
    "print(my_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-cable",
   "metadata": {},
   "source": [
    "### Get rid of html and perform other cleaning of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fifty-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code appears in many sites on internet and I took it and modified it for the stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function that receive a list of words and do lemmatization:\n",
    "def lemma_stem_text(words_list):\n",
    "    # Lemmatizer\n",
    "    # eighties->eight or messages->message or drugs->drug\n",
    "    text = [lemmatizer.lemmatize(token.lower()) for token in words_list]\n",
    "    # going-> go or started->start or watching->watch\n",
    "    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text]\n",
    "    return text\n",
    "\n",
    "re_negation = re.compile(\"n't \")\n",
    "\n",
    "# function that receive a sequence of woords and return the same sequence transforming\n",
    "# abbreviated negations to the standard form.\n",
    "def negation_abbreviated_to_standard(sent):\n",
    "    sent = re_negation.sub(\" not \", sent)\n",
    "    return sent\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # 1. Remove HTML tags\n",
    "#    review_text = BeautifulSoup(raw_review).get_text() \n",
    "\n",
    "    # 2. Transform abbreviated negations to the standard form.\n",
    "    review_text = negation_abbreviated_to_standard(raw_review)\n",
    "\n",
    "    # 3. Remove non-letters and non-numbers   \n",
    "    letters_numbers_only = re.sub(\"[^a-zA-Z_0-9]\", \" \", review_text) \n",
    "    \n",
    "    # 4. Convert to lower case and split into individual words (tokenization)\n",
    "    words = np.char.lower(letters_numbers_only.split())                             \n",
    "\n",
    "    # 5. Remove stop words\n",
    "    # stop words not used in final version\n",
    "#    meaningful_words = [w for w in words if not w in my_stop_words]   \n",
    "    meaningful_words = [w for w in words]   \n",
    "\n",
    "    # 6. Apply lemmatization function\n",
    "    lemma_words = lemma_stem_text(meaningful_words)\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join(lemma_words))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "based-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>8.0</td>\n",
       "      <td>In Panic In The Streets Richard Widmark plays ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If you ask me the first one was really better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>10.0</td>\n",
       "      <td>I am a big fan a Faerie Tale Theatre and I've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13640</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I just finished reading a book about Dillinger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Greg Davis and Bryan Daly take some crazed sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24897</th>\n",
       "      <td>3.0</td>\n",
       "      <td>This was not a well done western. You've got t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>8.0</td>\n",
       "      <td>I took this out arbitrarily from the library t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10352</th>\n",
       "      <td>9.0</td>\n",
       "      <td>To watch this film from start to finish withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>As with many other pop-culture franchise serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21131</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Filmed in a documentary style, but you can pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review\n",
       "6868      8.0  In Panic In The Streets Richard Widmark plays ...\n",
       "24016     1.0  If you ask me the first one was really better ...\n",
       "9668     10.0  I am a big fan a Faerie Tale Theatre and I've ...\n",
       "13640     1.0  I just finished reading a book about Dillinger...\n",
       "14018     2.0  Greg Davis and Bryan Daly take some crazed sta...\n",
       "...       ...                                                ...\n",
       "24897     3.0  This was not a well done western. You've got t...\n",
       "1084      8.0  I took this out arbitrarily from the library t...\n",
       "10352     9.0  To watch this film from start to finish withou...\n",
       "24197     1.0  As with many other pop-culture franchise serie...\n",
       "21131     3.0  Filmed in a documentary style, but you can pre...\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "embedded-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1562</th>\n",
       "      <th>1563</th>\n",
       "      <th>1564</th>\n",
       "      <th>1565</th>\n",
       "      <th>1566</th>\n",
       "      <th>1567</th>\n",
       "      <th>1568</th>\n",
       "      <th>1569</th>\n",
       "      <th>1570</th>\n",
       "      <th>1571</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in</td>\n",
       "      <td>panic</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>street</td>\n",
       "      <td>richard</td>\n",
       "      <td>widmark</td>\n",
       "      <td>play</td>\n",
       "      <td>u</td>\n",
       "      <td>s</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if</td>\n",
       "      <td>you</td>\n",
       "      <td>ask</td>\n",
       "      <td>me</td>\n",
       "      <td>the</td>\n",
       "      <td>first</td>\n",
       "      <td>one</td>\n",
       "      <td>wa</td>\n",
       "      <td>really</td>\n",
       "      <td>better</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>be</td>\n",
       "      <td>a</td>\n",
       "      <td>big</td>\n",
       "      <td>fan</td>\n",
       "      <td>a</td>\n",
       "      <td>faerie</td>\n",
       "      <td>tale</td>\n",
       "      <td>theatre</td>\n",
       "      <td>and</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>just</td>\n",
       "      <td>finish</td>\n",
       "      <td>read</td>\n",
       "      <td>a</td>\n",
       "      <td>book</td>\n",
       "      <td>about</td>\n",
       "      <td>dillinger</td>\n",
       "      <td>this</td>\n",
       "      <td>movie</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greg</td>\n",
       "      <td>davis</td>\n",
       "      <td>and</td>\n",
       "      <td>bryan</td>\n",
       "      <td>daly</td>\n",
       "      <td>take</td>\n",
       "      <td>some</td>\n",
       "      <td>craze</td>\n",
       "      <td>statement</td>\n",
       "      <td>by</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>this</td>\n",
       "      <td>wa</td>\n",
       "      <td>not</td>\n",
       "      <td>a</td>\n",
       "      <td>well</td>\n",
       "      <td>do</td>\n",
       "      <td>western</td>\n",
       "      <td>you</td>\n",
       "      <td>ve</td>\n",
       "      <td>get</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>i</td>\n",
       "      <td>take</td>\n",
       "      <td>this</td>\n",
       "      <td>out</td>\n",
       "      <td>arbitrarily</td>\n",
       "      <td>from</td>\n",
       "      <td>the</td>\n",
       "      <td>library</td>\n",
       "      <td>the</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>to</td>\n",
       "      <td>watch</td>\n",
       "      <td>this</td>\n",
       "      <td>film</td>\n",
       "      <td>from</td>\n",
       "      <td>start</td>\n",
       "      <td>to</td>\n",
       "      <td>finish</td>\n",
       "      <td>without</td>\n",
       "      <td>burst</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>a</td>\n",
       "      <td>with</td>\n",
       "      <td>many</td>\n",
       "      <td>other</td>\n",
       "      <td>pop</td>\n",
       "      <td>culture</td>\n",
       "      <td>franchise</td>\n",
       "      <td>series</td>\n",
       "      <td>this</td>\n",
       "      <td>line</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>film</td>\n",
       "      <td>in</td>\n",
       "      <td>a</td>\n",
       "      <td>documentary</td>\n",
       "      <td>style</td>\n",
       "      <td>but</td>\n",
       "      <td>you</td>\n",
       "      <td>can</td>\n",
       "      <td>pretty</td>\n",
       "      <td>well</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 1572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2            3            4        5          6     \\\n",
       "0       in  panic      in          the       street  richard    widmark   \n",
       "1       if    you     ask           me          the    first        one   \n",
       "2        i     be       a          big          fan        a     faerie   \n",
       "3        i   just  finish         read            a     book      about   \n",
       "4     greg  davis     and        bryan         daly     take       some   \n",
       "...    ...    ...     ...          ...          ...      ...        ...   \n",
       "2995  this     wa     not            a         well       do    western   \n",
       "2996     i   take    this          out  arbitrarily     from        the   \n",
       "2997    to  watch    this         film         from    start         to   \n",
       "2998     a   with    many        other          pop  culture  franchise   \n",
       "2999  film     in       a  documentary        style      but        you   \n",
       "\n",
       "           7          8       9     ...  1562  1563  1564  1565  1566  1567  \\\n",
       "0          play          u       s  ...  None  None  None  None  None  None   \n",
       "1            wa     really  better  ...  None  None  None  None  None  None   \n",
       "2          tale    theatre     and  ...  None  None  None  None  None  None   \n",
       "3     dillinger       this   movie  ...  None  None  None  None  None  None   \n",
       "4         craze  statement      by  ...  None  None  None  None  None  None   \n",
       "...         ...        ...     ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "2995        you         ve     get  ...  None  None  None  None  None  None   \n",
       "2996    library        the   other  ...  None  None  None  None  None  None   \n",
       "2997     finish    without   burst  ...  None  None  None  None  None  None   \n",
       "2998     series       this    line  ...  None  None  None  None  None  None   \n",
       "2999        can     pretty    well  ...  None  None  None  None  None  None   \n",
       "\n",
       "      1568  1569  1570  1571  \n",
       "0     None  None  None  None  \n",
       "1     None  None  None  None  \n",
       "2     None  None  None  None  \n",
       "3     None  None  None  None  \n",
       "4     None  None  None  None  \n",
       "...    ...   ...   ...   ...  \n",
       "2995  None  None  None  None  \n",
       "2996  None  None  None  None  \n",
       "2997  None  None  None  None  \n",
       "2998  None  None  None  None  \n",
       "2999  None  None  None  None  \n",
       "\n",
       "[3000 rows x 1572 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = train_sents.review.apply(review_to_words).str.split()\n",
    "words = pd.DataFrame(words.tolist())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "smoking-python",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Pinhead\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "D:\\Users\\Pinhead\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see how long (word count) are the reviews\n",
    "is_missing = words.isna().values\n",
    "# first Nan is the location where the first None appears \n",
    "# hence, the end of the review.\n",
    "first_nan = np.where(is_missing.any(1), is_missing.argmax(1), np.nan)\n",
    "plt.hist(first_nan,bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-homeless",
   "metadata": {},
   "source": [
    "### Create a new dataframe like \"words\" but with an additional column that represents the number of non-Nan columns in each row. Call this column frst_nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "biological-murder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1563</th>\n",
       "      <th>1564</th>\n",
       "      <th>1565</th>\n",
       "      <th>1566</th>\n",
       "      <th>1567</th>\n",
       "      <th>1568</th>\n",
       "      <th>1569</th>\n",
       "      <th>1570</th>\n",
       "      <th>1571</th>\n",
       "      <th>frst_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in</td>\n",
       "      <td>panic</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>street</td>\n",
       "      <td>richard</td>\n",
       "      <td>widmark</td>\n",
       "      <td>play</td>\n",
       "      <td>u</td>\n",
       "      <td>s</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if</td>\n",
       "      <td>you</td>\n",
       "      <td>ask</td>\n",
       "      <td>me</td>\n",
       "      <td>the</td>\n",
       "      <td>first</td>\n",
       "      <td>one</td>\n",
       "      <td>wa</td>\n",
       "      <td>really</td>\n",
       "      <td>better</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>be</td>\n",
       "      <td>a</td>\n",
       "      <td>big</td>\n",
       "      <td>fan</td>\n",
       "      <td>a</td>\n",
       "      <td>faerie</td>\n",
       "      <td>tale</td>\n",
       "      <td>theatre</td>\n",
       "      <td>and</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>just</td>\n",
       "      <td>finish</td>\n",
       "      <td>read</td>\n",
       "      <td>a</td>\n",
       "      <td>book</td>\n",
       "      <td>about</td>\n",
       "      <td>dillinger</td>\n",
       "      <td>this</td>\n",
       "      <td>movie</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greg</td>\n",
       "      <td>davis</td>\n",
       "      <td>and</td>\n",
       "      <td>bryan</td>\n",
       "      <td>daly</td>\n",
       "      <td>take</td>\n",
       "      <td>some</td>\n",
       "      <td>craze</td>\n",
       "      <td>statement</td>\n",
       "      <td>by</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>this</td>\n",
       "      <td>wa</td>\n",
       "      <td>not</td>\n",
       "      <td>a</td>\n",
       "      <td>well</td>\n",
       "      <td>do</td>\n",
       "      <td>western</td>\n",
       "      <td>you</td>\n",
       "      <td>ve</td>\n",
       "      <td>get</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>i</td>\n",
       "      <td>take</td>\n",
       "      <td>this</td>\n",
       "      <td>out</td>\n",
       "      <td>arbitrarily</td>\n",
       "      <td>from</td>\n",
       "      <td>the</td>\n",
       "      <td>library</td>\n",
       "      <td>the</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>to</td>\n",
       "      <td>watch</td>\n",
       "      <td>this</td>\n",
       "      <td>film</td>\n",
       "      <td>from</td>\n",
       "      <td>start</td>\n",
       "      <td>to</td>\n",
       "      <td>finish</td>\n",
       "      <td>without</td>\n",
       "      <td>burst</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>a</td>\n",
       "      <td>with</td>\n",
       "      <td>many</td>\n",
       "      <td>other</td>\n",
       "      <td>pop</td>\n",
       "      <td>culture</td>\n",
       "      <td>franchise</td>\n",
       "      <td>series</td>\n",
       "      <td>this</td>\n",
       "      <td>line</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>film</td>\n",
       "      <td>in</td>\n",
       "      <td>a</td>\n",
       "      <td>documentary</td>\n",
       "      <td>style</td>\n",
       "      <td>but</td>\n",
       "      <td>you</td>\n",
       "      <td>can</td>\n",
       "      <td>pretty</td>\n",
       "      <td>well</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 1573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2            3            4        5          6  \\\n",
       "0       in  panic      in          the       street  richard    widmark   \n",
       "1       if    you     ask           me          the    first        one   \n",
       "2        i     be       a          big          fan        a     faerie   \n",
       "3        i   just  finish         read            a     book      about   \n",
       "4     greg  davis     and        bryan         daly     take       some   \n",
       "...    ...    ...     ...          ...          ...      ...        ...   \n",
       "2995  this     wa     not            a         well       do    western   \n",
       "2996     i   take    this          out  arbitrarily     from        the   \n",
       "2997    to  watch    this         film         from    start         to   \n",
       "2998     a   with    many        other          pop  culture  franchise   \n",
       "2999  film     in       a  documentary        style      but        you   \n",
       "\n",
       "              7          8       9  ...  1563  1564  1565  1566  1567  1568  \\\n",
       "0          play          u       s  ...  None  None  None  None  None  None   \n",
       "1            wa     really  better  ...  None  None  None  None  None  None   \n",
       "2          tale    theatre     and  ...  None  None  None  None  None  None   \n",
       "3     dillinger       this   movie  ...  None  None  None  None  None  None   \n",
       "4         craze  statement      by  ...  None  None  None  None  None  None   \n",
       "...         ...        ...     ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "2995        you         ve     get  ...  None  None  None  None  None  None   \n",
       "2996    library        the   other  ...  None  None  None  None  None  None   \n",
       "2997     finish    without   burst  ...  None  None  None  None  None  None   \n",
       "2998     series       this    line  ...  None  None  None  None  None  None   \n",
       "2999        can     pretty    well  ...  None  None  None  None  None  None   \n",
       "\n",
       "      1569  1570  1571 frst_nan  \n",
       "0     None  None  None      474  \n",
       "1     None  None  None      139  \n",
       "2     None  None  None      152  \n",
       "3     None  None  None      169  \n",
       "4     None  None  None      145  \n",
       "...    ...   ...   ...      ...  \n",
       "2995  None  None  None       99  \n",
       "2996  None  None  None      185  \n",
       "2997  None  None  None      306  \n",
       "2998  None  None  None      140  \n",
       "2999  None  None  None      109  \n",
       "\n",
       "[3000 rows x 1573 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_fl = pd.concat([words,\n",
    "                      pd.Series(first_nan,\n",
    "                                name='frst_nan',\n",
    "                                dtype=int)\n",
    "                      .fillna(value=words.shape[1]-1)\n",
    "                      .astype(int)], axis=1)\n",
    "words_fl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-mistress",
   "metadata": {},
   "source": [
    "## Define some functions for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "different-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_row(row, sntc_ln=100):\n",
    "    \"\"\"\n",
    "    From each row (review) take \n",
    "    the first and last sentence (word\n",
    "    count) as defined by sntc_ln.\n",
    "    \"\"\"\n",
    "    # get the first occurence of None\n",
    "    # for this row.\n",
    "    frst_nan = row['frst_nan']\n",
    "    # if review is less than two \n",
    "    # sentences long, take the\n",
    "    # columns up to 2 * sentence length\n",
    "    if frst_nan <= 2*sntc_ln:\n",
    "        return row.iloc[:int(2*sntc_ln)].values\n",
    "    # else ...\n",
    "    else:\n",
    "        # grab first sentence length\n",
    "        sen1 = row.iloc[:sntc_ln]\n",
    "        # grab the last sentence length\n",
    "        sen2 = row.iloc[frst_nan-sntc_ln-1:frst_nan-1]\n",
    "        return np.concatenate((sen1,sen2))\n",
    "\n",
    "def soft_get(w):\n",
    "    \"\"\"\n",
    "    Convert a single word to a \n",
    "    word vector as provided by\n",
    "    the loaded gensim model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # retrieve the word vector if it exists\n",
    "        return word_vectors[w]\n",
    "    except KeyError:\n",
    "        # fill the vector space with zeros otherwise\n",
    "        # - justified because we are summing the \n",
    "        #   word vectors in map_vectors\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "\n",
    "def map_vectors(row, take_mean=True):\n",
    "    \"\"\"\n",
    "    Convert a row of words (with some NaN's)\n",
    "    to the sum of these word vectors\n",
    "    according to the gensim model.\n",
    "    \"\"\"\n",
    "    if np.sum(row.notna()) == 0:\n",
    "        # if entirely zeros, return zeros\n",
    "        # - but this should be eliminated \n",
    "        #   at some point to avoid singular matrix\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "    if take_mean:\n",
    "        try:\n",
    "            # otherwise, return the mean of the \n",
    "            # word vectors\n",
    "            return np.mean(\n",
    "                row.loc[row.notna()].apply(soft_get)\n",
    "            )\n",
    "        except:\n",
    "            # if error return zeros\n",
    "            return np.zeros(word_vectors.vector_size)\n",
    "    else:\n",
    "        try:\n",
    "            # otherwise, return the sum of the \n",
    "            # word vectors\n",
    "            return np.sum(\n",
    "                row.loc[row.notna()].apply(soft_get)\n",
    "            )\n",
    "        except:\n",
    "            # if error return zeros\n",
    "            return np.zeros(word_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-positive",
   "metadata": {},
   "source": [
    "## Define a class for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "neutral-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectSentences(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Create a class of inheritance from \n",
    "    BaseEstimator and TransformerMixin in\n",
    "    order to get the .fit and .transform\n",
    "    methods.\n",
    "    This will allow us to perform feature\n",
    "    engineering with these methods and\n",
    "    then use the class in a pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, use_gensim=True, \n",
    "                 take_mean=True, \n",
    "                 stnd_rows=False, \n",
    "                 sntc_len=100): # no *args or **kargs\n",
    "        self.sntc_len = sntc_len # length of sentence to use for 1st and last\n",
    "        self.take_mean = take_mean # whether to take mean of sum of vectors for gensim\n",
    "        self.stnd_rows = stnd_rows # whether to standardize rows or not for gensim\n",
    "        self.use_gensim = use_gensim # whether of not to use gensim\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Do all the feature engineering\n",
    "        # we want to do here.\n",
    "        # In our case, the function was\n",
    "        # already defined. Just call it.\n",
    "        \n",
    "        # create the series where only\n",
    "        # the required # of words in each\n",
    "        # row are retained.\n",
    "        X_sntc = X.apply(shift_row, \n",
    "                         args=(self.sntc_len,), \n",
    "                         axis=1)\n",
    "        if self.use_gensim:\n",
    "            X_sntc = pd.DataFrame(X_sntc.to_list())\n",
    "        else:\n",
    "            X_sntc = pd.DataFrame(X_sntc)\n",
    "            X_sntc = X_sntc[0].agg(lambda x: ','.join(map(str, x)).replace(',',' ')).to_list()\n",
    "        \n",
    "        if self.use_gensim:\n",
    "            # Apply the word vector transformation\n",
    "            # using either the sum or mean of \n",
    "            # the word vectors.\n",
    "            X_sntc = pd.DataFrame(X_sntc.apply(map_vectors, \n",
    "                                               args=(self.take_mean,),\n",
    "                                               axis=1)\n",
    "                                  .tolist())\n",
    "            # Finally, take the MinMaxScaler of \n",
    "            # each row if called for.\n",
    "            X_sntc = X_sntc.to_numpy()\n",
    "            if self.stnd_rows:\n",
    "                scalar = MinMaxScaler()\n",
    "                scalar.fit(np.transpose(X_sntc))\n",
    "                X_sntc = scalar.transform(np.transpose(X_sntc))\n",
    "                X_sntc = np.transpose(X_sntc)\n",
    "        return X_sntc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-printing",
   "metadata": {},
   "source": [
    "### Create the X and y and split into X_train and X_test variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "incredible-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = words_fl.copy()\n",
    "y_cat = to_categorical(train_sents['rating']-1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bizarre-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat the data step-by-step with the functions\n",
    "# defined above\n",
    "SelSent = SelectSentences(sntc_len=500, \n",
    "                          stnd_rows=False, \n",
    "                          take_mean=False, \n",
    "                          use_gensim=False)\n",
    "SelSent.fit(X)\n",
    "X = SelSent.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "western-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFidf = TfidfVectorizer(max_df=0.95, \n",
    "                        max_features=500, \n",
    "                        min_df=1, \n",
    "                        ngram_range=(1, 2), \n",
    "                        smooth_idf=False,\n",
    "                        stop_words=None, \n",
    "                        sublinear_tf=True)\n",
    "TFidf.fit(X)\n",
    "X = TFidf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "worse-parker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 500)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "indirect-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "alive-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Input(shape=(500,)),\n",
    "  Dense(units=500, activation='relu'),\n",
    "  Dense(units=256, activation='relu'),\n",
    "  Dense(units=192, activation='relu'),\n",
    "  Dense(units=128, activation='relu'),\n",
    "  Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "young-bulletin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               128256    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 192)               49344     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 454,094\n",
      "Trainable params: 454,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "secret-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "modular-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(train_sents['rating']-1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "seeing-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.9128\n",
      "Epoch 2/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.5471\n",
      "Epoch 3/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.3678\n",
      "Epoch 4/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.1631\n",
      "Epoch 5/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.8132\n",
      "Epoch 6/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.4988\n",
      "Epoch 7/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3019\n",
      "Epoch 8/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1661\n",
      "Epoch 9/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0782\n",
      "Epoch 10/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0487\n",
      "Epoch 11/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0206\n",
      "Epoch 12/12\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dcdb043d60>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X.todense(), \n",
    "          y=y_cat, \n",
    "          epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aware-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = df_test.review.apply(review_to_words).str.split()\n",
    "test_words = pd.DataFrame(test_words.tolist())\n",
    "is_missing = test_words.isna().values\n",
    "first_nan = np.where(is_missing.any(1), is_missing.argmax(1), np.nan)\n",
    "X_tt = pd.concat([test_words,\n",
    "                      pd.Series(first_nan,\n",
    "                                name='frst_nan',\n",
    "                                dtype=int)\n",
    "                      .fillna(value=test_words.shape[1]-1)\n",
    "                      .astype(int)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "employed-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat the data step-by-step with the functions\n",
    "# defined above\n",
    "SelSent = SelectSentences(sntc_len=500, \n",
    "                          stnd_rows=False, \n",
    "                          take_mean=False, \n",
    "                          use_gensim=False)\n",
    "SelSent.fit(X_tt)\n",
    "X_tt = SelSent.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "checked-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFidf = TfidfVectorizer(max_df=0.95, \n",
    "                        max_features=500, \n",
    "                        min_df=1, \n",
    "                        ngram_range=(1, 2), \n",
    "                        smooth_idf=False,\n",
    "                        stop_words=None, \n",
    "                        sublinear_tf=True)\n",
    "TFidf.fit(X_tt)\n",
    "X_tt = TFidf.transform(X_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "typical-chick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "suffering-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vec = model.predict(X_tt.todense(), \n",
    "                           batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bigger-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_vec,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "missing-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17892"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test['rating'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-armor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "occasional-chosen",
   "metadata": {},
   "source": [
    "# 2. (evil) XOR Problem\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "\n",
    "### 2.1 \n",
    "\n",
    "Generate a dataset of random <=100,000 binary strings of equal length <= 50. Train the LSTM; what is the maximum length you can train up to with precisison?\n",
    "    \n",
    "\n",
    "### 2.2\n",
    "\n",
    "Generate a dataset of random <=200,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "technical-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-seminar",
   "metadata": {},
   "source": [
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dynamic-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "N_SMPL = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improving-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary sequence of desired length\n",
    "data = np.array([[random.choice([0,1]) for _ in range(SEQ_LEN)] for _ in range(N_SMPL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "french-greek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 50, 1), (100000, 50, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_data = np.array([[int(not(x)) for x in row] for row in data])\n",
    "data = data[:,:,np.newaxis]\n",
    "not_data = not_data[:,:,np.newaxis]\n",
    "data.shape, not_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "popular-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.concatenate((data,not_data),axis=2)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fewer-challenge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0,:,0], data[0,:,1] # the random data and its negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the running XOR value by taking the modulo\n",
    "# of 2 of the cummulative sum of the array\n",
    "xor_run1 = np.array([[x % 2 for x in np.cumsum(row)] for row in data[:,:,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-privilege",
   "metadata": {},
   "source": [
    "### The key point to this method appears to be that one feeds the sequence (say, X) and its inverse (X_inv) into the model. At the same time, the sequence is accompanied by its true response (Y) whereas the inverse is accompanied by simply the inverse of the true response (Y_inv) instead of the true response of X_inv. The logic is that the lstm will learn to attribute correctly to a given sequence as distinguished from the inverse of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opening-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_run2 = np.array([[int(not(x)) for x in row] for row in xor_run1])\n",
    "xor_run1 = xor_run1[:,:,np.newaxis]\n",
    "xor_run2 = xor_run2[:,:,np.newaxis]\n",
    "xor_run = np.concatenate((xor_run1, xor_run2),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coral-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor_run.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "visible-madrid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 0]),\n",
       " array([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the running XOR of the sequence above (in data)\n",
    "xor_run[0,:,0], xor_run[0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gorgeous-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN,2),dtype=\"float32\"))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "agreed-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 1)             16        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50, 2)             4         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-tender",
   "metadata": {},
   "source": [
    "### Length 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "arabic-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6929 - accuracy: 0.5094\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6651 - accuracy: 0.5614\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1180 - accuracy: 0.9936\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0403 - accuracy: 0.9985\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0220 - accuracy: 0.9993\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0138 - accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0093 - accuracy: 0.9998\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0064 - accuracy: 0.9999\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0046 - accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0034 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd83481280>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=data.astype(np.float32), y=xor_run.astype(np.float32), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-sentence",
   "metadata": {},
   "source": [
    "### Length 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marked-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6930 - accuracy: 0.5068\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6418 - accuracy: 0.6110\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1779 - accuracy: 0.9989\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0801 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0479 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1be7ffae700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "SEQ_LEN = 35\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN,2),dtype=\"float32\"))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x=data[:,:SEQ_LEN,:].astype(np.float32), \n",
    "          y=xor_run[:,:SEQ_LEN,:].astype(np.float32), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-tunnel",
   "metadata": {},
   "source": [
    "### Length 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assisted-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6941 - accuracy: 0.4824\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6930 - accuracy: 0.5106\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6924 - accuracy: 0.5155\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6640 - accuracy: 0.5785\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1711 - accuracy: 0.9970\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0678 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0126 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bec4616ac0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "SEQ_LEN = 25\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN,2),dtype=\"float32\"))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x=data[:,:SEQ_LEN,:].astype(np.float32), \n",
    "          y=xor_run[:,:SEQ_LEN,:].astype(np.float32), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-province",
   "metadata": {},
   "source": [
    "### Length 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "academic-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6879 - accuracy: 0.5288\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5841 - accuracy: 0.6923\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1993 - accuracy: 0.9931\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0884 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0516 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1be7e2d1910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "SEQ_LEN = 15\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN,2),dtype=\"float32\"))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x=data[:,:SEQ_LEN,:].astype(np.float32), \n",
    "          y=xor_run[:,:SEQ_LEN,:].astype(np.float32), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-fluid",
   "metadata": {},
   "source": [
    "### The model fits tremendously well all the way to a size of 50. \"Fits\" is defined as the XOR value of the full sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-hollow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "greatest-defensive",
   "metadata": {},
   "source": [
    "## Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "modular-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, None, 1)           16        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, None, 2)           4         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 100_000\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(None,2),dtype=\"float32\"))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-guarantee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "golden-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6960 - accuracy: 0.4881\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6928 - accuracy: 0.5159\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6900 - accuracy: 0.5155\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6865 - accuracy: 0.5141\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6760 - accuracy: 0.5330\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6416 - accuracy: 0.5814\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2277 - accuracy: 0.9594\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0850 - accuracy: 0.9937\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0428 - accuracy: 0.9988\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0309 - accuracy: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bee218fe50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        sequence_len = np.random.randint(1, 50)\n",
    "        data = np.array([random.choice([0,1]) for _ in range(sequence_len)])\n",
    "        not_data = np.array([int(not(x)) for x in data])\n",
    "        data = data[:,np.newaxis]\n",
    "        not_data = not_data[:,np.newaxis]\n",
    "        x_train = np.concatenate((data,not_data),axis=1)\n",
    "        xor_run1 = np.array([x % 2 for x in np.cumsum(x_train[:,0])])\n",
    "        xor_run2 = np.array([int(not(x)) for x in xor_run1])\n",
    "        xor_run1 = xor_run1[:,np.newaxis]\n",
    "        xor_run2 = xor_run2[:,np.newaxis]\n",
    "        y_train = np.concatenate((xor_run1, xor_run2),axis=1)\n",
    "        x_train = x_train[np.newaxis, :, :]\n",
    "        y_train = y_train[np.newaxis, :, :]\n",
    "        #print(x_train.shape, y_train.shape)\n",
    "        yield x_train.astype(np.float32), y_train.astype(np.float32)\n",
    "\n",
    "model.fit_generator(train_generator(), steps_per_epoch=782, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-malaysia",
   "metadata": {},
   "source": [
    "### Let's bump it up to 200_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rocky-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, None, 1)           16        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, None, 2)           4         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 200_000\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(None,2),dtype=\"float32\"))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "collaborative-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6930 - accuracy: 0.5003\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6923 - accuracy: 0.5106\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6904 - accuracy: 0.5191\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6895 - accuracy: 0.5134\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6820 - accuracy: 0.5250\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6618 - accuracy: 0.5534\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5800 - accuracy: 0.6735\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2705 - accuracy: 0.9660\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1188 - accuracy: 0.9928\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0603 - accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bee53f8670>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        sequence_len = np.random.randint(1, 50)\n",
    "        data = np.array([random.choice([0,1]) for _ in range(sequence_len)])\n",
    "        not_data = np.array([int(not(x)) for x in data])\n",
    "        data = data[:,np.newaxis]\n",
    "        not_data = not_data[:,np.newaxis]\n",
    "        x_train = np.concatenate((data,not_data),axis=1)\n",
    "        xor_run1 = np.array([x % 2 for x in np.cumsum(x_train[:,0])])\n",
    "        xor_run2 = np.array([int(not(x)) for x in xor_run1])\n",
    "        xor_run1 = xor_run1[:,np.newaxis]\n",
    "        xor_run2 = xor_run2[:,np.newaxis]\n",
    "        y_train = np.concatenate((xor_run1, xor_run2),axis=1)\n",
    "        x_train = x_train[np.newaxis, :, :]\n",
    "        y_train = y_train[np.newaxis, :, :]\n",
    "        #print(x_train.shape, y_train.shape)\n",
    "        yield x_train.astype(np.float32), y_train.astype(np.float32)\n",
    "\n",
    "model.fit_generator(train_generator(), steps_per_epoch=782, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-korea",
   "metadata": {},
   "source": [
    "## There appears to be no loss of accuracy with this method even with variable vector lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-slovenia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
